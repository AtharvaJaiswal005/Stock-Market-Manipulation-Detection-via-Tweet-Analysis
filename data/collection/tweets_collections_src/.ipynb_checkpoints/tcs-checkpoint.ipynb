{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f375fd8b-5a6f-4d14-bebe-5e31f483100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from random import randint\n",
    "from twikit import Client, TooManyRequests\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30bd6d97-019d-4121-bc36-9c06db3cdd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets collected so far: 10\n",
      "Total tweets collected so far: 20\n",
      "Total tweets collected so far: 30\n",
      "Total tweets collected so far: 40\n",
      "Total tweets collected so far: 50\n",
      "Total tweets collected so far: 60\n",
      "Total tweets collected so far: 70\n",
      "Total tweets collected so far: 70\n",
      "Total tweets collected so far: 80\n",
      "Total tweets collected so far: 90\n",
      "Total tweets collected so far: 100\n",
      "Total tweets collected so far: 110\n",
      "Total tweets collected so far: 120\n",
      "Total tweets collected so far: 130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for 15 minutes:   1%|â–Œ                                                          | 9/900 [00:10<16:33,  1.11s/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 76\u001b[0m, in \u001b[0;36mcollect_tweets_for_day\u001b[1;34m(date)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m     tweets \u001b[38;5;241m=\u001b[39m \u001b[43mget_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtweets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TooManyRequests \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[7], line 34\u001b[0m, in \u001b[0;36mget_tweets\u001b[1;34m(query, tweets)\u001b[0m\n\u001b[0;32m     33\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGetting tweets for query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m     tweets \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_tweet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\twikit\\client.py:660\u001b[0m, in \u001b[0;36mClient.search_tweet\u001b[1;34m(self, query, product, count, cursor)\u001b[0m\n\u001b[0;32m    658\u001b[0m product \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mcapitalize()\n\u001b[1;32m--> 660\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    661\u001b[0m instructions \u001b[38;5;241m=\u001b[39m find_dict(response, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m, find_one\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\twikit\\client.py:602\u001b[0m, in \u001b[0;36mClient._search\u001b[1;34m(self, query, product, count, cursor)\u001b[0m\n\u001b[0;32m    598\u001b[0m params \u001b[38;5;241m=\u001b[39m flatten_params({\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: variables,\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: FEATURES\n\u001b[0;32m    601\u001b[0m })\n\u001b[1;32m--> 602\u001b[0m response, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEndpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSEARCH_TIMELINE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_headers\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\twikit\\client.py:169\u001b[0m, in \u001b[0;36mBaseClient.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m|\u001b[39m Any, httpx\u001b[38;5;241m.\u001b[39mResponse]:\n\u001b[1;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\twikit\\client.py:160\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[1;34m(self, method, url, auto_unlock, raise_exception, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AccountSuspended(message, headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests(message, headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n",
      "\u001b[1;31mTooManyRequests\u001b[0m: status: 429, message: \"Rate limit exceeded\n\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 88\u001b[0m, in \u001b[0;36mcollect_tweets_for_day\u001b[1;34m(date)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m     tweets \u001b[38;5;241m=\u001b[39m \u001b[43mget_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtweets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSuccessfully resumed after switching cookies.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 34\u001b[0m, in \u001b[0;36mget_tweets\u001b[1;34m(query, tweets)\u001b[0m\n\u001b[0;32m     33\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGetting tweets for query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m     tweets \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_tweet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\twikit\\client.py:660\u001b[0m, in \u001b[0;36mClient.search_tweet\u001b[1;34m(self, query, product, count, cursor)\u001b[0m\n\u001b[0;32m    658\u001b[0m product \u001b[38;5;241m=\u001b[39m product\u001b[38;5;241m.\u001b[39mcapitalize()\n\u001b[1;32m--> 660\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    661\u001b[0m instructions \u001b[38;5;241m=\u001b[39m find_dict(response, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m'\u001b[39m, find_one\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\twikit\\client.py:602\u001b[0m, in \u001b[0;36mClient._search\u001b[1;34m(self, query, product, count, cursor)\u001b[0m\n\u001b[0;32m    598\u001b[0m params \u001b[38;5;241m=\u001b[39m flatten_params({\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvariables\u001b[39m\u001b[38;5;124m'\u001b[39m: variables,\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m: FEATURES\n\u001b[0;32m    601\u001b[0m })\n\u001b[1;32m--> 602\u001b[0m response, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEndpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSEARCH_TIMELINE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_base_headers\u001b[49m\n\u001b[0;32m    606\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\twikit\\client.py:169\u001b[0m, in \u001b[0;36mBaseClient.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m|\u001b[39m Any, httpx\u001b[38;5;241m.\u001b[39mResponse]:\n\u001b[1;32m--> 169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\twikit\\client.py:160\u001b[0m, in \u001b[0;36mBaseClient.request\u001b[1;34m(self, method, url, auto_unlock, raise_exception, **kwargs)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AccountSuspended(message, headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[1;32m--> 160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TooManyRequests(message, headers\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m status_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m600\u001b[39m:\n",
      "\u001b[1;31mTooManyRequests\u001b[0m: status: 429, message: \"Rate limit exceeded\n\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 155\u001b[0m\n\u001b[0;32m    153\u001b[0m current_date \u001b[38;5;241m=\u001b[39m start_date\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current_date \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m END_DATE:\n\u001b[1;32m--> 155\u001b[0m     \u001b[43mcollect_tweets_for_day\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     current_date \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished collecting tweets. Total tweets collected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_tweets_collected\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 97\u001b[0m, in \u001b[0;36mcollect_tweets_for_day\u001b[1;34m(date)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     96\u001b[0m             logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRate limit reset time is in the past. Waiting for 15 minutes as a precaution.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 97\u001b[0m             \u001b[43mdelay_for_minutes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 15-minute delay in case of past reset time\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ReadTimeout:\n",
      "Cell \u001b[1;32mIn[7], line 65\u001b[0m, in \u001b[0;36mdelay_for_minutes\u001b[1;34m(minutes)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39mwait_time, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWaiting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminutes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m minutes\u001b[39m\u001b[38;5;124m'\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m rate_pbar:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(wait_time):\n\u001b[1;32m---> 65\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m         rate_pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from random import randint\n",
    "from twikit import Client, TooManyRequests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from requests.exceptions import ReadTimeout\n",
    "\n",
    "# Configuration\n",
    "MINIMUM_TWEETS = 10\n",
    "START_DATE = datetime(2022, 1, 1)  # Start date of the range\n",
    "END_DATE = datetime(2023, 12, 31)  # End date of the range\n",
    "DELAY_BETWEEN_REQUESTS = 1  # Delay between requests in seconds\n",
    "CSV_FILE = 'TCS.csv'  # Updated file for TCS data\n",
    "SUMMARY_FILE = 'TCS_Collection_Summary.csv'  # Updated summary file for TCS\n",
    "\n",
    "# Initialize Twitter client with the first cookie set\n",
    "client = Client(timeout=10)  # Set timeout to 10 seconds\n",
    "current_cookies = 'cookies.json'\n",
    "client.load_cookies(current_cookies)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='tcs_tweet_collection.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize counters\n",
    "total_tweets_collected = 0\n",
    "daily_stats = []\n",
    "\n",
    "def get_tweets(query, tweets):\n",
    "    if tweets is None:\n",
    "        logging.info(f'Getting tweets for query: {query}...')\n",
    "        tweets = client.search_tweet(query, product='Top')\n",
    "    else:\n",
    "        wait_time = randint(5, 10)\n",
    "        logging.info(f'Getting next tweets after {wait_time} seconds ...')\n",
    "        time.sleep(wait_time)\n",
    "        tweets = tweets.next()\n",
    "\n",
    "    return tweets\n",
    "\n",
    "def construct_query(date):\n",
    "    next_day = date + timedelta(days=1)\n",
    "    query = (f'(TCS OR \"Tata Consultancy Services\" OR #TCS OR #TataConsultancy OR '\n",
    "             f'Stock OR Market OR #StockMarket OR #MarketAnalysis) '\n",
    "             f'lang:en until:{next_day.strftime(\"%Y-%m-%d\")} since:{date.strftime(\"%Y-%m-%d\")}')\n",
    "    return query\n",
    "\n",
    "def switch_cookies():\n",
    "    global current_cookies\n",
    "    if current_cookies == 'cookies.json':\n",
    "        current_cookies = 'cookies2.json'\n",
    "    else:\n",
    "        current_cookies = 'cookies.json'\n",
    "    \n",
    "    logging.info(f'Switching to {current_cookies} due to rate limit...')\n",
    "    client.load_cookies(current_cookies)\n",
    "\n",
    "def delay_for_minutes(minutes=5):\n",
    "    \"\"\"Delays for the specified number of minutes, showing progress with tqdm.\"\"\"\n",
    "    wait_time = minutes * 60  # Convert minutes to seconds\n",
    "    with tqdm(total=wait_time, desc=f'Waiting for {minutes} minutes', unit='s', leave=True) as rate_pbar:\n",
    "        for _ in range(wait_time):\n",
    "            time.sleep(1)\n",
    "            rate_pbar.update(1)\n",
    "\n",
    "def collect_tweets_for_day(date):\n",
    "    global total_tweets_collected\n",
    "    query = construct_query(date)\n",
    "    tweet_count = 0\n",
    "    tweets = None\n",
    "    \n",
    "    while tweet_count < MINIMUM_TWEETS:\n",
    "        try:\n",
    "            tweets = get_tweets(query, tweets)\n",
    "        except TooManyRequests as e:\n",
    "            rate_limit_reset = datetime.fromtimestamp(e.rate_limit_reset)\n",
    "            current_time = datetime.now()\n",
    "            wait_time = (rate_limit_reset - current_time).total_seconds()\n",
    "            \n",
    "            # Switch cookies and retry\n",
    "            logging.warning(f'Rate limit reached. Switching cookies.')\n",
    "            switch_cookies()\n",
    "            \n",
    "            # Immediately retry with new cookies\n",
    "            try:\n",
    "                tweets = get_tweets(query, tweets)\n",
    "                logging.info('Successfully resumed after switching cookies.')\n",
    "            except TooManyRequests as e:\n",
    "                logging.warning(f'Rate limit still in place even after switching cookies.')\n",
    "                if wait_time > 0:\n",
    "                    logging.warning(f'Waiting until {rate_limit_reset}')\n",
    "                    delay_for_minutes(wait_time // 60)  # Convert wait_time to minutes for tqdm\n",
    "                else:\n",
    "                    logging.warning(f'Rate limit reset time is in the past. Waiting for 15 minutes as a precaution.')\n",
    "                    delay_for_minutes(5)  # 15-minute delay in case of past reset time\n",
    "            continue\n",
    "\n",
    "        except ReadTimeout:\n",
    "            logging.error(f'ReadTimeout occurred. Retrying in a few seconds...')\n",
    "            time.sleep(5)  # Small delay before retrying\n",
    "            continue  # Retry the loop to fetch the tweets again\n",
    "\n",
    "        if not tweets:\n",
    "            logging.info(f'No more tweets found for {date.strftime(\"%Y-%m-%d\")}')\n",
    "            break\n",
    "\n",
    "        for tweet in tweets:\n",
    "            if tweet_count >= MINIMUM_TWEETS:\n",
    "                break\n",
    "            tweet_count += 1\n",
    "            total_tweets_collected += 1\n",
    "            tweet_data = [date.strftime('%Y-%m-%d'), tweet_count, tweet.user.name, tweet.text, tweet.created_at, tweet.retweet_count, tweet.favorite_count]\n",
    "            \n",
    "            with open(CSV_FILE, 'a', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(tweet_data)\n",
    "\n",
    "        logging.info(f'Got {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}')\n",
    "        daily_stats.append([date.strftime('%Y-%m-%d'), tweet_count])\n",
    "    \n",
    "    if tweet_count < MINIMUM_TWEETS:\n",
    "        logging.warning(f'Collected {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}, which is less than the minimum of {MINIMUM_TWEETS}')\n",
    "    \n",
    "    logging.info(f'Done for {date.strftime(\"%Y-%m-%d\")}')\n",
    "    print(f'Total tweets collected so far: {total_tweets_collected}')\n",
    "\n",
    "def get_last_collected_date():\n",
    "    \"\"\"Reads the CSV file to find the last collected date.\"\"\"\n",
    "    if not os.path.exists(CSV_FILE):\n",
    "        return START_DATE\n",
    "    with open(CSV_FILE, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "        if len(rows) > 1:\n",
    "            last_date = rows[-1][0]\n",
    "            return datetime.strptime(last_date, '%Y-%m-%d') + timedelta(days=1)\n",
    "    return START_DATE\n",
    "\n",
    "# Write header to the CSV file if it doesn't exist\n",
    "if not os.path.exists(CSV_FILE):\n",
    "    with open(CSV_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweet Count', 'Username', 'Text', 'Created At', 'Retweets', 'Likes'])\n",
    "\n",
    "# Get the starting date\n",
    "start_date = get_last_collected_date()\n",
    "if start_date > END_DATE:\n",
    "    print(f'All data has already been collected up to {END_DATE.strftime(\"%Y-%m-%d\")}.')\n",
    "else:\n",
    "    # Collect tweets for the specified date range\n",
    "    current_date = start_date\n",
    "    while current_date <= END_DATE:\n",
    "        collect_tweets_for_day(current_date)\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    print(f'Finished collecting tweets. Total tweets collected: {total_tweets_collected}')\n",
    "\n",
    "    # Save summary to a CSV file\n",
    "    with open(SUMMARY_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweets Collected'])\n",
    "        writer.writerows(daily_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb326375-09be-4359-a717-4590f484ff5c",
   "metadata": {},
   "source": [
    "# HDFC Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25972d8f-d968-4c08-ac79-df7ec5dac861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from random import randint\n",
    "from twikit import Client, TooManyRequests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "MINIMUM_TWEETS = 10\n",
    "START_DATE = datetime(2022, 1, 1)  # Start date of the range\n",
    "END_DATE = datetime(2023, 12, 31)  # End date of the range\n",
    "DELAY_BETWEEN_REQUESTS = 1  # Delay between requests in seconds\n",
    "CSV_FILE = 'HDFC_Bank.csv'  # File to store HDFC Bank tweets\n",
    "SUMMARY_FILE = 'HDFC_Collection_Summary.csv'  # File to store collection summary\n",
    "\n",
    "# Initialize Twitter client\n",
    "client = Client()  # Initialize your Twitter client\n",
    "client.load_cookies('cookies.json')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='tweet_collection.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize counters\n",
    "total_tweets_collected = 0\n",
    "daily_stats = []\n",
    "\n",
    "def get_tweets(query, tweets):\n",
    "    if tweets is None:\n",
    "        logging.info(f'Getting tweets for query: {query}...')\n",
    "        tweets = client.search_tweet(query, product='Top')\n",
    "    else:\n",
    "        wait_time = randint(5, 10)\n",
    "        logging.info(f'Getting next tweets after {wait_time} seconds ...')\n",
    "        time.sleep(wait_time)\n",
    "        tweets = tweets.next()\n",
    "\n",
    "    return tweets\n",
    "\n",
    "def construct_query(date):\n",
    "    next_day = date + timedelta(days=1)\n",
    "    query = (f'(HDFC OR \"HDFC Bank\" OR #HDFC OR #HDFCBank OR '\n",
    "             f'Stock OR Market OR #StockMarket OR #MarketAnalysis) '\n",
    "             f'lang:en until:{next_day.strftime(\"%Y-%m-%d\")} since:{date.strftime(\"%Y-%m-%d\")}')\n",
    "    return query\n",
    "\n",
    "def delay_for_minutes(minutes=15):\n",
    "    \"\"\"Delays for the specified number of minutes, showing progress with tqdm.\"\"\"\n",
    "    wait_time = minutes * 60  # Convert minutes to seconds\n",
    "    with tqdm(total=wait_time, desc=f'Waiting for {minutes} minutes', unit='s', leave=True) as rate_pbar:\n",
    "        for _ in range(wait_time):\n",
    "            time.sleep(1)\n",
    "            rate_pbar.update(1)\n",
    "\n",
    "def collect_tweets_for_day(date):\n",
    "    global total_tweets_collected\n",
    "    query = construct_query(date)\n",
    "    tweet_count = 0\n",
    "    tweets = None\n",
    "    \n",
    "    while tweet_count < MINIMUM_TWEETS:\n",
    "        try:\n",
    "            tweets = get_tweets(query, tweets)\n",
    "        except TooManyRequests as e:\n",
    "            rate_limit_reset = datetime.fromtimestamp(e.rate_limit_reset)\n",
    "            current_time = datetime.now()\n",
    "            wait_time = (rate_limit_reset - current_time).total_seconds()\n",
    "            if wait_time > 0:\n",
    "                logging.warning(f'Rate limit reached. Waiting until {rate_limit_reset}')\n",
    "                delay_for_minutes(wait_time // 60)  # Convert wait_time to minutes for tqdm\n",
    "            else:\n",
    "                logging.warning(f'Rate limit reset time is in the past. Continuing immediately.')\n",
    "                delay_for_minutes(15)  # 15-minute delay in case of past reset time\n",
    "            continue\n",
    "\n",
    "        if not tweets:\n",
    "            logging.info(f'No more tweets found for {date.strftime(\"%Y-%m-%d\")}')\n",
    "            break\n",
    "\n",
    "        for tweet in tweets:\n",
    "            if tweet_count >= MINIMUM_TWEETS:\n",
    "                break\n",
    "            tweet_count += 1\n",
    "            total_tweets_collected += 1\n",
    "            tweet_data = [date.strftime('%Y-%m-%d'), tweet_count, tweet.user.name, tweet.text, tweet.created_at, tweet.retweet_count, tweet.favorite_count]\n",
    "            \n",
    "            with open(CSV_FILE, 'a', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(tweet_data)\n",
    "\n",
    "        logging.info(f'Got {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}')\n",
    "        daily_stats.append([date.strftime('%Y-%m-%d'), tweet_count])\n",
    "    \n",
    "    if tweet_count < MINIMUM_TWEETS:\n",
    "        logging.warning(f'Collected {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}, which is less than the minimum of {MINIMUM_TWEETS}')\n",
    "    \n",
    "    logging.info(f'Done for {date.strftime(\"%Y-%m-%d\")}')\n",
    "    print(f'Total tweets collected so far: {total_tweets_collected}')\n",
    "\n",
    "def get_last_collected_date():\n",
    "    \"\"\"Reads the CSV file to find the last collected date.\"\"\"\n",
    "    if not os.path.exists(CSV_FILE):\n",
    "        return START_DATE\n",
    "    with open(CSV_FILE, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "        if len(rows) > 1:\n",
    "            last_date = rows[-1][0]\n",
    "            return datetime.strptime(last_date, '%Y-%m-%d') + timedelta(days=1)\n",
    "    return START_DATE\n",
    "\n",
    "# Write header to the CSV file if it doesn't exist\n",
    "if not os.path.exists(CSV_FILE):\n",
    "    with open(CSV_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweet Count', 'Username', 'Text', 'Created At', 'Retweets', 'Likes'])\n",
    "\n",
    "# Get the starting date\n",
    "start_date = get_last_collected_date()\n",
    "if start_date > END_DATE:\n",
    "    print(f'All data has already been collected up to {END_DATE.strftime(\"%Y-%m-%d\")}.')\n",
    "else:\n",
    "    # Collect tweets for the specified date range\n",
    "    current_date = start_date\n",
    "    while current_date <= END_DATE:\n",
    "        collect_tweets_for_day(current_date)\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    print(f'Finished collecting tweets. Total tweets collected: {total_tweets_collected}')\n",
    "\n",
    "    # Save summary to a CSV file\n",
    "    with open(SUMMARY_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweets Collected'])\n",
    "        writer.writerows(daily_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893fcf66-d0ab-4a13-9414-fe6114278aa1",
   "metadata": {},
   "source": [
    "# INFY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233aaa2f-9b44-4afe-b43a-e431db36ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from random import randint\n",
    "from twikit import Client, TooManyRequests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "MINIMUM_TWEETS = 10\n",
    "START_DATE = datetime(2022, 1, 1)  # Start date of the range\n",
    "END_DATE = datetime(2023, 12, 31)  # End date of the range\n",
    "DELAY_BETWEEN_REQUESTS = 1  # Delay between requests in seconds\n",
    "CSV_FILE = 'INFY.csv'  # File to store Infosys (INFY) tweets\n",
    "SUMMARY_FILE = 'INFY_Collection_Summary.csv'  # File to store collection summary\n",
    "\n",
    "# Initialize Twitter client\n",
    "client = Client()  # Initialize your Twitter client\n",
    "client.load_cookies('cookies.json')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='tweet_collection.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize counters\n",
    "total_tweets_collected = 0\n",
    "daily_stats = []\n",
    "\n",
    "def get_tweets(query, tweets):\n",
    "    if tweets is None:\n",
    "        logging.info(f'Getting tweets for query: {query}...')\n",
    "        tweets = client.search_tweet(query, product='Top')\n",
    "    else:\n",
    "        wait_time = randint(5, 10)\n",
    "        logging.info(f'Getting next tweets after {wait_time} seconds ...')\n",
    "        time.sleep(wait_time)\n",
    "        tweets = tweets.next()\n",
    "\n",
    "    return tweets\n",
    "\n",
    "def construct_query(date):\n",
    "    next_day = date + timedelta(days=1)\n",
    "    query = (f'(Infosys OR INFY OR \"Infosys Ltd\" OR #Infosys OR #INFY OR '\n",
    "             f'Stock OR Market OR #StockMarket OR #MarketAnalysis) '\n",
    "             f'lang:en until:{next_day.strftime(\"%Y-%m-%d\")} since:{date.strftime(\"%Y-%m-%d\")}')\n",
    "    return query\n",
    "\n",
    "def delay_for_minutes(minutes=15):\n",
    "    \"\"\"Delays for the specified number of minutes, showing progress with tqdm.\"\"\"\n",
    "    wait_time = minutes * 60  # Convert minutes to seconds\n",
    "    with tqdm(total=wait_time, desc=f'Waiting for {minutes} minutes', unit='s', leave=True) as rate_pbar:\n",
    "        for _ in range(wait_time):\n",
    "            time.sleep(1)\n",
    "            rate_pbar.update(1)\n",
    "\n",
    "def collect_tweets_for_day(date):\n",
    "    global total_tweets_collected\n",
    "    query = construct_query(date)\n",
    "    tweet_count = 0\n",
    "    tweets = None\n",
    "    \n",
    "    while tweet_count < MINIMUM_TWEETS:\n",
    "        try:\n",
    "            tweets = get_tweets(query, tweets)\n",
    "        except TooManyRequests as e:\n",
    "            rate_limit_reset = datetime.fromtimestamp(e.rate_limit_reset)\n",
    "            current_time = datetime.now()\n",
    "            wait_time = (rate_limit_reset - current_time).total_seconds()\n",
    "            if wait_time > 0:\n",
    "                logging.warning(f'Rate limit reached. Waiting until {rate_limit_reset}')\n",
    "                delay_for_minutes(wait_time // 60)  # Convert wait_time to minutes for tqdm\n",
    "            else:\n",
    "                logging.warning(f'Rate limit reset time is in the past. Continuing immediately.')\n",
    "                delay_for_minutes(15)  # 15-minute delay in case of past reset time\n",
    "            continue\n",
    "\n",
    "        if not tweets:\n",
    "            logging.info(f'No more tweets found for {date.strftime(\"%Y-%m-%d\")}')\n",
    "            break\n",
    "\n",
    "        for tweet in tweets:\n",
    "            if tweet_count >= MINIMUM_TWEETS:\n",
    "                break\n",
    "            tweet_count += 1\n",
    "            total_tweets_collected += 1\n",
    "            tweet_data = [date.strftime('%Y-%m-%d'), tweet_count, tweet.user.name, tweet.text, tweet.created_at, tweet.retweet_count, tweet.favorite_count]\n",
    "            \n",
    "            with open(CSV_FILE, 'a', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(tweet_data)\n",
    "\n",
    "        logging.info(f'Got {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}')\n",
    "        daily_stats.append([date.strftime('%Y-%m-%d'), tweet_count])\n",
    "    \n",
    "    if tweet_count < MINIMUM_TWEETS:\n",
    "        logging.warning(f'Collected {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}, which is less than the minimum of {MINIMUM_TWEETS}')\n",
    "    \n",
    "    logging.info(f'Done for {date.strftime(\"%Y-%m-%d\")}')\n",
    "    print(f'Total tweets collected so far: {total_tweets_collected}')\n",
    "\n",
    "def get_last_collected_date():\n",
    "    \"\"\"Reads the CSV file to find the last collected date.\"\"\"\n",
    "    if not os.path.exists(CSV_FILE):\n",
    "        return START_DATE\n",
    "    with open(CSV_FILE, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "        if len(rows) > 1:\n",
    "            last_date = rows[-1][0]\n",
    "            return datetime.strptime(last_date, '%Y-%m-%d') + timedelta(days=1)\n",
    "    return START_DATE\n",
    "\n",
    "# Write header to the CSV file if it doesn't exist\n",
    "if not os.path.exists(CSV_FILE):\n",
    "    with open(CSV_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweet Count', 'Username', 'Text', 'Created At', 'Retweets', 'Likes'])\n",
    "\n",
    "# Get the starting date\n",
    "start_date = get_last_collected_date()\n",
    "if start_date > END_DATE:\n",
    "    print(f'All data has already been collected up to {END_DATE.strftime(\"%Y-%m-%d\")}.')\n",
    "else:\n",
    "    # Collect tweets for the specified date range\n",
    "    current_date = start_date\n",
    "    while current_date <= END_DATE:\n",
    "        collect_tweets_for_day(current_date)\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    print(f'Finished collecting tweets. Total tweets collected: {total_tweets_collected}')\n",
    "\n",
    "    # Save summary to a CSV file\n",
    "    with open(SUMMARY_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweets Collected'])\n",
    "        writer.writerows(daily_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a9eb7f-b67c-4d87-8d31-8641b65f8410",
   "metadata": {},
   "source": [
    "# ICICI Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f063232-132e-40e9-a6f6-770e37903f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from random import randint\n",
    "from twikit import Client, TooManyRequests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "MINIMUM_TWEETS = 10\n",
    "START_DATE = datetime(2022, 1, 1)  # Start date of the range\n",
    "END_DATE = datetime(2023, 12, 31)  # End date of the range\n",
    "DELAY_BETWEEN_REQUESTS = 1  # Delay between requests in seconds\n",
    "CSV_FILE = 'ICICIBANK.csv'  # File to store ICICI Bank tweets\n",
    "SUMMARY_FILE = 'ICICIBANK_Collection_Summary.csv'  # File to store collection summary\n",
    "\n",
    "# Initialize Twitter client\n",
    "client = Client()  # Initialize your Twitter client\n",
    "client.load_cookies('cookies.json')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='tweet_collection.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize counters\n",
    "total_tweets_collected = 0\n",
    "daily_stats = []\n",
    "\n",
    "def get_tweets(query, tweets):\n",
    "    if tweets is None:\n",
    "        logging.info(f'Getting tweets for query: {query}...')\n",
    "        tweets = client.search_tweet(query, product='Top')\n",
    "    else:\n",
    "        wait_time = randint(5, 10)\n",
    "        logging.info(f'Getting next tweets after {wait_time} seconds ...')\n",
    "        time.sleep(wait_time)\n",
    "        tweets = tweets.next()\n",
    "\n",
    "    return tweets\n",
    "\n",
    "def construct_query(date):\n",
    "    next_day = date + timedelta(days=1)\n",
    "    query = (f'(ICICI OR ICICIBank OR \"ICICI Bank\" OR #ICICIBank OR #ICICI OR '\n",
    "             f'Stock OR Market OR #StockMarket OR #MarketAnalysis) '\n",
    "             f'lang:en until:{next_day.strftime(\"%Y-%m-%d\")} since:{date.strftime(\"%Y-%m-%d\")}')\n",
    "    return query\n",
    "\n",
    "def delay_for_minutes(minutes=15):\n",
    "    \"\"\"Delays for the specified number of minutes, showing progress with tqdm.\"\"\"\n",
    "    wait_time = minutes * 60  # Convert minutes to seconds\n",
    "    with tqdm(total=wait_time, desc=f'Waiting for {minutes} minutes', unit='s', leave=True) as rate_pbar:\n",
    "        for _ in range(wait_time):\n",
    "            time.sleep(1)\n",
    "            rate_pbar.update(1)\n",
    "\n",
    "def collect_tweets_for_day(date):\n",
    "    global total_tweets_collected\n",
    "    query = construct_query(date)\n",
    "    tweet_count = 0\n",
    "    tweets = None\n",
    "    \n",
    "    while tweet_count < MINIMUM_TWEETS:\n",
    "        try:\n",
    "            tweets = get_tweets(query, tweets)\n",
    "        except TooManyRequests as e:\n",
    "            rate_limit_reset = datetime.fromtimestamp(e.rate_limit_reset)\n",
    "            current_time = datetime.now()\n",
    "            wait_time = (rate_limit_reset - current_time).total_seconds()\n",
    "            if wait_time > 0:\n",
    "                logging.warning(f'Rate limit reached. Waiting until {rate_limit_reset}')\n",
    "                delay_for_minutes(wait_time // 60)  # Convert wait_time to minutes for tqdm\n",
    "            else:\n",
    "                logging.warning(f'Rate limit reset time is in the past. Continuing immediately.')\n",
    "                delay_for_minutes(15)  # 15-minute delay in case of past reset time\n",
    "            continue\n",
    "\n",
    "        if not tweets:\n",
    "            logging.info(f'No more tweets found for {date.strftime(\"%Y-%m-%d\")}')\n",
    "            break\n",
    "\n",
    "        for tweet in tweets:\n",
    "            if tweet_count >= MINIMUM_TWEETS:\n",
    "                break\n",
    "            tweet_count += 1\n",
    "            total_tweets_collected += 1\n",
    "            tweet_data = [date.strftime('%Y-%m-%d'), tweet_count, tweet.user.name, tweet.text, tweet.created_at, tweet.retweet_count, tweet.favorite_count]\n",
    "            \n",
    "            with open(CSV_FILE, 'a', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(tweet_data)\n",
    "\n",
    "        logging.info(f'Got {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}')\n",
    "        daily_stats.append([date.strftime('%Y-%m-%d'), tweet_count])\n",
    "    \n",
    "    if tweet_count < MINIMUM_TWEETS:\n",
    "        logging.warning(f'Collected {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}, which is less than the minimum of {MINIMUM_TWEETS}')\n",
    "    \n",
    "    logging.info(f'Done for {date.strftime(\"%Y-%m-%d\")}')\n",
    "    print(f'Total tweets collected so far: {total_tweets_collected}')\n",
    "\n",
    "def get_last_collected_date():\n",
    "    \"\"\"Reads the CSV file to find the last collected date.\"\"\"\n",
    "    if not os.path.exists(CSV_FILE):\n",
    "        return START_DATE\n",
    "    with open(CSV_FILE, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "        if len(rows) > 1:\n",
    "            last_date = rows[-1][0]\n",
    "            return datetime.strptime(last_date, '%Y-%m-%d') + timedelta(days=1)\n",
    "    return START_DATE\n",
    "\n",
    "# Write header to the CSV file if it doesn't exist\n",
    "if not os.path.exists(CSV_FILE):\n",
    "    with open(CSV_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweet Count', 'Username', 'Text', 'Created At', 'Retweets', 'Likes'])\n",
    "\n",
    "# Get the starting date\n",
    "start_date = get_last_collected_date()\n",
    "if start_date > END_DATE:\n",
    "    print(f'All data has already been collected up to {END_DATE.strftime(\"%Y-%m-%d\")}.')\n",
    "else:\n",
    "    # Collect tweets for the specified date range\n",
    "    current_date = start_date\n",
    "    while current_date <= END_DATE:\n",
    "        collect_tweets_for_day(current_date)\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    print(f'Finished collecting tweets. Total tweets collected: {total_tweets_collected}')\n",
    "\n",
    "    # Save summary to a CSV file\n",
    "    with open(SUMMARY_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweets Collected'])\n",
    "        writer.writerows(daily_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0132ab-3400-4ff6-9ed2-f4a74eeae487",
   "metadata": {},
   "source": [
    "# HINDUNILVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e105160f-c9da-4589-b86f-1e8fe50712a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from random import randint\n",
    "from twikit import Client, TooManyRequests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "MINIMUM_TWEETS = 10\n",
    "START_DATE = datetime(2022, 1, 1)  # Start date of the range\n",
    "END_DATE = datetime(2023, 12, 31)  # End date of the range\n",
    "DELAY_BETWEEN_REQUESTS = 1  # Delay between requests in seconds\n",
    "CSV_FILE = 'HINDUNILVR.csv'  # File to store Hindustan Unilever tweets\n",
    "SUMMARY_FILE = 'HINDUNILVR_Collection_Summary.csv'  # File to store collection summary\n",
    "\n",
    "# Initialize Twitter client\n",
    "client = Client()  # Initialize your Twitter client\n",
    "client.load_cookies('cookies.json')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='tweet_collection.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize counters\n",
    "total_tweets_collected = 0\n",
    "daily_stats = []\n",
    "\n",
    "def get_tweets(query, tweets):\n",
    "    if tweets is None:\n",
    "        logging.info(f'Getting tweets for query: {query}...')\n",
    "        tweets = client.search_tweet(query, product='Top')\n",
    "    else:\n",
    "        wait_time = randint(5, 10)\n",
    "        logging.info(f'Getting next tweets after {wait_time} seconds ...')\n",
    "        time.sleep(wait_time)\n",
    "        tweets = tweets.next()\n",
    "\n",
    "    return tweets\n",
    "\n",
    "def construct_query(date):\n",
    "    next_day = date + timedelta(days=1)\n",
    "    query = (f'(HUL OR HINDUNILVR OR \"Hindustan Unilever\" OR #HUL OR #HINDUNILVR OR '\n",
    "             f'Stock OR Market OR #StockMarket OR #MarketAnalysis) '\n",
    "             f'lang:en until:{next_day.strftime(\"%Y-%m-%d\")} since:{date.strftime(\"%Y-%m-%d\")}')\n",
    "    return query\n",
    "\n",
    "def delay_for_minutes(minutes=15):\n",
    "    \"\"\"Delays for the specified number of minutes, showing progress with tqdm.\"\"\"\n",
    "    wait_time = minutes * 60  # Convert minutes to seconds\n",
    "    with tqdm(total=wait_time, desc=f'Waiting for {minutes} minutes', unit='s', leave=True) as rate_pbar:\n",
    "        for _ in range(wait_time):\n",
    "            time.sleep(1)\n",
    "            rate_pbar.update(1)\n",
    "\n",
    "def collect_tweets_for_day(date):\n",
    "    global total_tweets_collected\n",
    "    query = construct_query(date)\n",
    "    tweet_count = 0\n",
    "    tweets = None\n",
    "    \n",
    "    while tweet_count < MINIMUM_TWEETS:\n",
    "        try:\n",
    "            tweets = get_tweets(query, tweets)\n",
    "        except TooManyRequests as e:\n",
    "            rate_limit_reset = datetime.fromtimestamp(e.rate_limit_reset)\n",
    "            current_time = datetime.now()\n",
    "            wait_time = (rate_limit_reset - current_time).total_seconds()\n",
    "            if wait_time > 0:\n",
    "                logging.warning(f'Rate limit reached. Waiting until {rate_limit_reset}')\n",
    "                delay_for_minutes(wait_time // 60)  # Convert wait_time to minutes for tqdm\n",
    "            else:\n",
    "                logging.warning(f'Rate limit reset time is in the past. Continuing immediately.')\n",
    "                delay_for_minutes(15)  # 15-minute delay in case of past reset time\n",
    "            continue\n",
    "\n",
    "        if not tweets:\n",
    "            logging.info(f'No more tweets found for {date.strftime(\"%Y-%m-%d\")}')\n",
    "            break\n",
    "\n",
    "        for tweet in tweets:\n",
    "            if tweet_count >= MINIMUM_TWEETS:\n",
    "                break\n",
    "            tweet_count += 1\n",
    "            total_tweets_collected += 1\n",
    "            tweet_data = [date.strftime('%Y-%m-%d'), tweet_count, tweet.user.name, tweet.text, tweet.created_at, tweet.retweet_count, tweet.favorite_count]\n",
    "            \n",
    "            with open(CSV_FILE, 'a', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(tweet_data)\n",
    "\n",
    "        logging.info(f'Got {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}')\n",
    "        daily_stats.append([date.strftime('%Y-%m-%d'), tweet_count])\n",
    "    \n",
    "    if tweet_count < MINIMUM_TWEETS:\n",
    "        logging.warning(f'Collected {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}, which is less than the minimum of {MINIMUM_TWEETS}')\n",
    "    \n",
    "    logging.info(f'Done for {date.strftime(\"%Y-%m-%d\")}')\n",
    "    print(f'Total tweets collected so far: {total_tweets_collected}')\n",
    "\n",
    "def get_last_collected_date():\n",
    "    \"\"\"Reads the CSV file to find the last collected date.\"\"\"\n",
    "    if not os.path.exists(CSV_FILE):\n",
    "        return START_DATE\n",
    "    with open(CSV_FILE, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "        if len(rows) > 1:\n",
    "            last_date = rows[-1][0]\n",
    "            return datetime.strptime(last_date, '%Y-%m-%d') + timedelta(days=1)\n",
    "    return START_DATE\n",
    "\n",
    "# Write header to the CSV file if it doesn't exist\n",
    "if not os.path.exists(CSV_FILE):\n",
    "    with open(CSV_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweet Count', 'Username', 'Text', 'Created At', 'Retweets', 'Likes'])\n",
    "\n",
    "# Get the starting date\n",
    "start_date = get_last_collected_date()\n",
    "if start_date > END_DATE:\n",
    "    print(f'All data has already been collected up to {END_DATE.strftime(\"%Y-%m-%d\")}.')\n",
    "else:\n",
    "    # Collect tweets for the specified date range\n",
    "    current_date = start_date\n",
    "    while current_date <= END_DATE:\n",
    "        collect_tweets_for_day(current_date)\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    print(f'Finished collecting tweets. Total tweets collected: {total_tweets_collected}')\n",
    "\n",
    "    # Save summary to a CSV file\n",
    "    with open(SUMMARY_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweets Collected'])\n",
    "        writer.writerows(daily_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d7d05c-b6ad-4e0f-b781-15b82c2cd19d",
   "metadata": {},
   "source": [
    "# ITC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2525bef8-b595-4e3a-af7d-8f4e57a52e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from random import randint\n",
    "from twikit import Client, TooManyRequests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "MINIMUM_TWEETS = 10\n",
    "START_DATE = datetime(2022, 1, 1)  # Start date of the range\n",
    "END_DATE = datetime(2023, 12, 31)  # End date of the range\n",
    "DELAY_BETWEEN_REQUESTS = 1  # Delay between requests in seconds\n",
    "CSV_FILE = 'ITC.csv'  # File to store ITC tweets\n",
    "SUMMARY_FILE = 'ITC_Collection_Summary.csv'  # File to store collection summary\n",
    "\n",
    "# Initialize Twitter client\n",
    "client = Client()  # Initialize your Twitter client\n",
    "client.load_cookies('cookies.json')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='tweet_collection.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize counters\n",
    "total_tweets_collected = 0\n",
    "daily_stats = []\n",
    "\n",
    "def get_tweets(query, tweets):\n",
    "    if tweets is None:\n",
    "        logging.info(f'Getting tweets for query: {query}...')\n",
    "        tweets = client.search_tweet(query, product='Top')\n",
    "    else:\n",
    "        wait_time = randint(5, 10)\n",
    "        logging.info(f'Getting next tweets after {wait_time} seconds ...')\n",
    "        time.sleep(wait_time)\n",
    "        tweets = tweets.next()\n",
    "\n",
    "    return tweets\n",
    "\n",
    "def construct_query(date):\n",
    "    next_day = date + timedelta(days=1)\n",
    "    query = (f'(ITC OR #ITC OR \"ITC Ltd\" OR #ITCLimited OR '\n",
    "             f'Stock OR Market OR #StockMarket OR #MarketAnalysis) '\n",
    "             f'lang:en until:{next_day.strftime(\"%Y-%m-%d\")} since:{date.strftime(\"%Y-%m-%d\")}')\n",
    "    return query\n",
    "\n",
    "def delay_for_minutes(minutes=15):\n",
    "    \"\"\"Delays for the specified number of minutes, showing progress with tqdm.\"\"\"\n",
    "    wait_time = minutes * 60  # Convert minutes to seconds\n",
    "    with tqdm(total=wait_time, desc=f'Waiting for {minutes} minutes', unit='s', leave=True) as rate_pbar:\n",
    "        for _ in range(wait_time):\n",
    "            time.sleep(1)\n",
    "            rate_pbar.update(1)\n",
    "\n",
    "def collect_tweets_for_day(date):\n",
    "    global total_tweets_collected\n",
    "    query = construct_query(date)\n",
    "    tweet_count = 0\n",
    "    tweets = None\n",
    "    \n",
    "    while tweet_count < MINIMUM_TWEETS:\n",
    "        try:\n",
    "            tweets = get_tweets(query, tweets)\n",
    "        except TooManyRequests as e:\n",
    "            rate_limit_reset = datetime.fromtimestamp(e.rate_limit_reset)\n",
    "            current_time = datetime.now()\n",
    "            wait_time = (rate_limit_reset - current_time).total_seconds()\n",
    "            if wait_time > 0:\n",
    "                logging.warning(f'Rate limit reached. Waiting until {rate_limit_reset}')\n",
    "                delay_for_minutes(wait_time // 60)  # Convert wait_time to minutes for tqdm\n",
    "            else:\n",
    "                logging.warning(f'Rate limit reset time is in the past. Continuing immediately.')\n",
    "                delay_for_minutes(15)  # 15-minute delay in case of past reset time\n",
    "            continue\n",
    "\n",
    "        if not tweets:\n",
    "            logging.info(f'No more tweets found for {date.strftime(\"%Y-%m-%d\")}')\n",
    "            break\n",
    "\n",
    "        for tweet in tweets:\n",
    "            if tweet_count >= MINIMUM_TWEETS:\n",
    "                break\n",
    "            tweet_count += 1\n",
    "            total_tweets_collected += 1\n",
    "            tweet_data = [date.strftime('%Y-%m-%d'), tweet_count, tweet.user.name, tweet.text, tweet.created_at, tweet.retweet_count, tweet.favorite_count]\n",
    "            \n",
    "            with open(CSV_FILE, 'a', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(tweet_data)\n",
    "\n",
    "        logging.info(f'Got {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}')\n",
    "        daily_stats.append([date.strftime('%Y-%m-%d'), tweet_count])\n",
    "    \n",
    "    if tweet_count < MINIMUM_TWEETS:\n",
    "        logging.warning(f'Collected {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}, which is less than the minimum of {MINIMUM_TWEETS}')\n",
    "    \n",
    "    logging.info(f'Done for {date.strftime(\"%Y-%m-%d\")}')\n",
    "    print(f'Total tweets collected so far: {total_tweets_collected}')\n",
    "\n",
    "def get_last_collected_date():\n",
    "    \"\"\"Reads the CSV file to find the last collected date.\"\"\"\n",
    "    if not os.path.exists(CSV_FILE):\n",
    "        return START_DATE\n",
    "    with open(CSV_FILE, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "        if len(rows) > 1:\n",
    "            last_date = rows[-1][0]\n",
    "            return datetime.strptime(last_date, '%Y-%m-%d') + timedelta(days=1)\n",
    "    return START_DATE\n",
    "\n",
    "# Write header to the CSV file if it doesn't exist\n",
    "if not os.path.exists(CSV_FILE):\n",
    "    with open(CSV_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweet Count', 'Username', 'Text', 'Created At', 'Retweets', 'Likes'])\n",
    "\n",
    "# Get the starting date\n",
    "start_date = get_last_collected_date()\n",
    "if start_date > END_DATE:\n",
    "    print(f'All data has already been collected up to {END_DATE.strftime(\"%Y-%m-%d\")}.')\n",
    "else:\n",
    "    # Collect tweets for the specified date range\n",
    "    current_date = start_date\n",
    "    while current_date <= END_DATE:\n",
    "        collect_tweets_for_day(current_date)\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    print(f'Finished collecting tweets. Total tweets collected: {total_tweets_collected}')\n",
    "\n",
    "    # Save summary to a CSV file\n",
    "    with open(SUMMARY_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweets Collected'])\n",
    "        writer.writerows(daily_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebaba4c-c2c2-486e-a2b7-fdea282aec34",
   "metadata": {},
   "source": [
    "# SBIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5a75b6-6360-4589-9a65-b40ef3ac5dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from random import randint\n",
    "from twikit import Client, TooManyRequests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "MINIMUM_TWEETS = 10\n",
    "START_DATE = datetime(2022, 1, 1)  # Start date of the range\n",
    "END_DATE = datetime(2023, 12, 31)  # End date of the range\n",
    "DELAY_BETWEEN_REQUESTS = 1  # Delay between requests in seconds\n",
    "CSV_FILE = 'SBIN.csv'  # File to store SBIN tweets\n",
    "SUMMARY_FILE = 'SBIN_Collection_Summary.csv'  # File to store collection summary\n",
    "\n",
    "# Initialize Twitter client\n",
    "client = Client()  # Initialize your Twitter client\n",
    "client.load_cookies('cookies.json')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='tweet_collection.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize counters\n",
    "total_tweets_collected = 0\n",
    "daily_stats = []\n",
    "\n",
    "def get_tweets(query, tweets):\n",
    "    if tweets is None:\n",
    "        logging.info(f'Getting tweets for query: {query}...')\n",
    "        tweets = client.search_tweet(query, product='Top')\n",
    "    else:\n",
    "        wait_time = randint(5, 10)\n",
    "        logging.info(f'Getting next tweets after {wait_time} seconds ...')\n",
    "        time.sleep(wait_time)\n",
    "        tweets = tweets.next()\n",
    "\n",
    "    return tweets\n",
    "\n",
    "def construct_query(date):\n",
    "    next_day = date + timedelta(days=1)\n",
    "    query = (f'(SBIN OR \"State Bank of India\" OR #SBI OR #SBIN OR \"SBI Bank\" OR '\n",
    "             f'Stock OR Market OR #StockMarket OR #MarketAnalysis) '\n",
    "             f'lang:en until:{next_day.strftime(\"%Y-%m-%d\")} since:{date.strftime(\"%Y-%m-%d\")}')\n",
    "    return query\n",
    "\n",
    "def delay_for_minutes(minutes=15):\n",
    "    \"\"\"Delays for the specified number of minutes, showing progress with tqdm.\"\"\"\n",
    "    wait_time = minutes * 60  # Convert minutes to seconds\n",
    "    with tqdm(total=wait_time, desc=f'Waiting for {minutes} minutes', unit='s', leave=True) as rate_pbar:\n",
    "        for _ in range(wait_time):\n",
    "            time.sleep(1)\n",
    "            rate_pbar.update(1)\n",
    "\n",
    "def collect_tweets_for_day(date):\n",
    "    global total_tweets_collected\n",
    "    query = construct_query(date)\n",
    "    tweet_count = 0\n",
    "    tweets = None\n",
    "    \n",
    "    while tweet_count < MINIMUM_TWEETS:\n",
    "        try:\n",
    "            tweets = get_tweets(query, tweets)\n",
    "        except TooManyRequests as e:\n",
    "            rate_limit_reset = datetime.fromtimestamp(e.rate_limit_reset)\n",
    "            current_time = datetime.now()\n",
    "            wait_time = (rate_limit_reset - current_time).total_seconds()\n",
    "            if wait_time > 0:\n",
    "                logging.warning(f'Rate limit reached. Waiting until {rate_limit_reset}')\n",
    "                delay_for_minutes(wait_time // 60)  # Convert wait_time to minutes for tqdm\n",
    "            else:\n",
    "                logging.warning(f'Rate limit reset time is in the past. Continuing immediately.')\n",
    "                delay_for_minutes(15)  # 15-minute delay in case of past reset time\n",
    "            continue\n",
    "\n",
    "        if not tweets:\n",
    "            logging.info(f'No more tweets found for {date.strftime(\"%Y-%m-%d\")}')\n",
    "            break\n",
    "\n",
    "        for tweet in tweets:\n",
    "            if tweet_count >= MINIMUM_TWEETS:\n",
    "                break\n",
    "            tweet_count += 1\n",
    "            total_tweets_collected += 1\n",
    "            tweet_data = [date.strftime('%Y-%m-%d'), tweet_count, tweet.user.name, tweet.text, tweet.created_at, tweet.retweet_count, tweet.favorite_count]\n",
    "            \n",
    "            with open(CSV_FILE, 'a', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(tweet_data)\n",
    "\n",
    "        logging.info(f'Got {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}')\n",
    "        daily_stats.append([date.strftime('%Y-%m-%d'), tweet_count])\n",
    "    \n",
    "    if tweet_count < MINIMUM_TWEETS:\n",
    "        logging.warning(f'Collected {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}, which is less than the minimum of {MINIMUM_TWEETS}')\n",
    "    \n",
    "    logging.info(f'Done for {date.strftime(\"%Y-%m-%d\")}')\n",
    "    print(f'Total tweets collected so far: {total_tweets_collected}')\n",
    "\n",
    "def get_last_collected_date():\n",
    "    \"\"\"Reads the CSV file to find the last collected date.\"\"\"\n",
    "    if not os.path.exists(CSV_FILE):\n",
    "        return START_DATE\n",
    "    with open(CSV_FILE, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "        if len(rows) > 1:\n",
    "            last_date = rows[-1][0]\n",
    "            return datetime.strptime(last_date, '%Y-%m-%d') + timedelta(days=1)\n",
    "    return START_DATE\n",
    "\n",
    "# Write header to the CSV file if it doesn't exist\n",
    "if not os.path.exists(CSV_FILE):\n",
    "    with open(CSV_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweet Count', 'Username', 'Text', 'Created At', 'Retweets', 'Likes'])\n",
    "\n",
    "# Get the starting date\n",
    "start_date = get_last_collected_date()\n",
    "if start_date > END_DATE:\n",
    "    print(f'All data has already been collected up to {END_DATE.strftime(\"%Y-%m-%d\")}.')\n",
    "else:\n",
    "    # Collect tweets for the specified date range\n",
    "    current_date = start_date\n",
    "    while current_date <= END_DATE:\n",
    "        collect_tweets_for_day(current_date)\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    print(f'Finished collecting tweets. Total tweets collected: {total_tweets_collected}')\n",
    "\n",
    "    # Save summary to a CSV file\n",
    "    with open(SUMMARY_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweets Collected'])\n",
    "        writer.writerows(daily_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc27ab91-3c70-4e0f-a47b-58c215437946",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf1df802-6cd2-4604-80aa-e8fd679fa222",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install twikit==1.7.6 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d320be8-df94-41d3-b3b3-54c78d81986e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "642d64c8-34de-4382-b90e-1465100c85d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twikit import Client, TooManyRequests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from configparser import ConfigParser\n",
    "from random import randint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "858aba56-c404-453f-aa51-8a98ddb3d2e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data has already been collected up to 2023-12-31.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from random import randint\n",
    "from twikit import Client, TooManyRequests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "MINIMUM_TWEETS = 10\n",
    "START_DATE = datetime(2022, 1, 1)  # Start date of the range\n",
    "END_DATE = datetime(2023, 12, 31)  # End date of the range\n",
    "DELAY_BETWEEN_REQUESTS = 1  # Delay between requests in seconds\n",
    "CSV_FILE = 'Reliance_Industry.csv'\n",
    "SUMMARY_FILE = 'Collection_Summary.csv'\n",
    "\n",
    "# Initialize Twitter client\n",
    "client = Client()  # Initialize your Twitter client\n",
    "client.load_cookies('cookies.json')\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='tweet_collection.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Initialize counters\n",
    "total_tweets_collected = 0\n",
    "daily_stats = []\n",
    "\n",
    "def get_tweets(query, tweets):\n",
    "    if tweets is None:\n",
    "        logging.info(f'Getting tweets for query: {query}...')\n",
    "        tweets = client.search_tweet(query, product='Top')\n",
    "    else:\n",
    "        wait_time = randint(5, 10)\n",
    "        logging.info(f'Getting next tweets after {wait_time} seconds ...')\n",
    "        time.sleep(wait_time)\n",
    "        tweets = tweets.next()\n",
    "\n",
    "    return tweets\n",
    "\n",
    "def construct_query(date):\n",
    "    next_day = date + timedelta(days=1)\n",
    "    query = (f'(Reliance OR \"Reliance Industries\" OR #Reliance OR #RelianceIndustries OR '\n",
    "             f'Stock OR Market OR #StockMarket OR #MarketAnalysis) '\n",
    "             f'lang:en until:{next_day.strftime(\"%Y-%m-%d\")} since:{date.strftime(\"%Y-%m-%d\")}')\n",
    "    return query\n",
    "\n",
    "def delay_for_minutes(minutes=15):\n",
    "    \"\"\"Delays for the specified number of minutes, showing progress with tqdm.\"\"\"\n",
    "    wait_time = minutes * 60  # Convert minutes to seconds\n",
    "    with tqdm(total=wait_time, desc=f'Waiting for {minutes} minutes', unit='s', leave=True) as rate_pbar:\n",
    "        for _ in range(wait_time):\n",
    "            time.sleep(1)\n",
    "            rate_pbar.update(1)\n",
    "\n",
    "def collect_tweets_for_day(date):\n",
    "    global total_tweets_collected\n",
    "    query = construct_query(date)\n",
    "    tweet_count = 0\n",
    "    tweets = None\n",
    "    \n",
    "    while tweet_count < MINIMUM_TWEETS:\n",
    "        try:\n",
    "            tweets = get_tweets(query, tweets)\n",
    "        except TooManyRequests as e:\n",
    "            rate_limit_reset = datetime.fromtimestamp(e.rate_limit_reset)\n",
    "            current_time = datetime.now()\n",
    "            wait_time = (rate_limit_reset - current_time).total_seconds()\n",
    "            if wait_time > 0:\n",
    "                logging.warning(f'Rate limit reached. Waiting until {rate_limit_reset}')\n",
    "                delay_for_minutes(wait_time // 60)  # Convert wait_time to minutes for tqdm\n",
    "            else:\n",
    "                logging.warning(f'Rate limit reset time is in the past. Continuing immediately.')\n",
    "                delay_for_minutes(15)  # 15-minute delay in case of past reset time\n",
    "            continue\n",
    "\n",
    "        if not tweets:\n",
    "            logging.info(f'No more tweets found for {date.strftime(\"%Y-%m-%d\")}')\n",
    "            break\n",
    "\n",
    "        for tweet in tweets:\n",
    "            if tweet_count >= MINIMUM_TWEETS:\n",
    "                break\n",
    "            tweet_count += 1\n",
    "            total_tweets_collected += 1\n",
    "            tweet_data = [date.strftime('%Y-%m-%d'), tweet_count, tweet.user.name, tweet.text, tweet.created_at, tweet.retweet_count, tweet.favorite_count]\n",
    "            \n",
    "            with open(CSV_FILE, 'a', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(tweet_data)\n",
    "\n",
    "        logging.info(f'Got {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}')\n",
    "        daily_stats.append([date.strftime('%Y-%m-%d'), tweet_count])\n",
    "    \n",
    "    if tweet_count < MINIMUM_TWEETS:\n",
    "        logging.warning(f'Collected {tweet_count} tweets for {date.strftime(\"%Y-%m-%d\")}, which is less than the minimum of {MINIMUM_TWEETS}')\n",
    "    \n",
    "    logging.info(f'Done for {date.strftime(\"%Y-%m-%d\")}')\n",
    "    print(f'Total tweets collected so far: {total_tweets_collected}')\n",
    "\n",
    "def get_last_collected_date():\n",
    "    \"\"\"Reads the CSV file to find the last collected date.\"\"\"\n",
    "    if not os.path.exists(CSV_FILE):\n",
    "        return START_DATE\n",
    "    with open(CSV_FILE, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        rows = list(reader)\n",
    "        if len(rows) > 1:\n",
    "            last_date = rows[-1][0]\n",
    "            return datetime.strptime(last_date, '%Y-%m-%d') + timedelta(days=1)\n",
    "    return START_DATE\n",
    "\n",
    "# Write header to the CSV file if it doesn't exist\n",
    "if not os.path.exists(CSV_FILE):\n",
    "    with open(CSV_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweet Count', 'Username', 'Text', 'Created At', 'Retweets', 'Likes'])\n",
    "\n",
    "# Get the starting date\n",
    "start_date = get_last_collected_date()\n",
    "if start_date > END_DATE:\n",
    "    print(f'All data has already been collected up to {END_DATE.strftime(\"%Y-%m-%d\")}.')\n",
    "else:\n",
    "    # Collect tweets for the specified date range\n",
    "    current_date = start_date\n",
    "    while current_date <= END_DATE:\n",
    "        collect_tweets_for_day(current_date)\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    print(f'Finished collecting tweets. Total tweets collected: {total_tweets_collected}')\n",
    "\n",
    "    # Save summary to a CSV file\n",
    "    with open(SUMMARY_FILE, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Date', 'Tweets Collected'])\n",
    "        writer.writerows(daily_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c0bb31b-d187-4b83-b33a-a888be614b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#* login credentials\n",
    "username = \"AtharvaKHW\"\n",
    "email = \"atharv.jaiswal@choithraminternational.com\"\n",
    "password = \"Student@123\"\n",
    "\n",
    "#* create a csv file with 'Date Collected' as the first column\n",
    "with open('tweets.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Date Collected', 'Tweet_count', 'Username', 'Text', 'Created At', 'Retweets', 'Likes'])\n",
    "\n",
    "#* authenticate to X.com\n",
    "client = Client(language='en-US')\n",
    "# client.login(auth_info_1=username, auth_info_2=email, password=password)\n",
    "# client.save_cookies('cookies.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e0a6b6-af19-4f2a-bf6c-6e081e0027ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

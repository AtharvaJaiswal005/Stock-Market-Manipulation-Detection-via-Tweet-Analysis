{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6680c8fd-2a59-4714-b2c3-bf6da5fa84cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from scraper import collect_tweets_for_day, get_last_collected_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "614f1ec3-8136-4abd-9127-7bd6568b24ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data for ITC.NS has already been collected up to 2023-12-31.\n",
      "All data for SBIN.NS has already been collected up to 2023-12-31.\n",
      "All data for KOTAKBANK.NS has already been collected up to 2023-12-31.\n",
      "All data for BHARTIARTL.NS has already been collected up to 2023-12-31.\n",
      "All data for HCLTECH.NS has already been collected up to 2023-12-31.\n",
      "All data for LT.NS has already been collected up to 2023-12-31.\n",
      "All data for AXISBANK.NS has already been collected up to 2023-12-31.\n",
      "All data for ASIANPAINT.NS has already been collected up to 2023-12-31.\n",
      "All data for BAJFINANCE.NS has already been collected up to 2023-12-31.\n",
      "All data for MARUTI.NS has already been collected up to 2023-12-31.\n",
      "All data for M&M.NS has already been collected up to 2023-12-31.\n",
      "All data for SUNPHARMA.NS has already been collected up to 2023-12-31.\n",
      "All data for TITAN.NS has already been collected up to 2023-12-31.\n",
      "All data for ULTRACEMCO.NS has already been collected up to 2023-12-31.\n",
      "All data for NESTLEIND.NS has already been collected up to 2023-12-31.\n",
      "All data for INDUSINDBK.NS has already been collected up to 2023-12-31.\n",
      "All data for ADANIENT.NS has already been collected up to 2023-12-31.\n",
      "All data for POWERGRID.NS has already been collected up to 2023-12-31.\n",
      "All data for NTPC.NS has already been collected up to 2023-12-31.\n",
      "All data for TATASTEEL.NS has already been collected up to 2023-12-31.\n",
      "All data for JSWSTEEL.NS has already been collected up to 2023-12-31.\n",
      "All data for ONGC.NS has already been collected up to 2023-12-31.\n",
      "All data for BAJAJFINSV.NS has already been collected up to 2023-12-31.\n",
      "All data for DIVISLAB.NS has already been collected up to 2023-12-31.\n",
      "All data for TECHM.NS has already been collected up to 2023-12-31.\n",
      "All data for WIPRO.NS has already been collected up to 2023-12-31.\n",
      "All data for GRASIM.NS has already been collected up to 2023-12-31.\n",
      "All data for BRITANNIA.NS has already been collected up to 2023-12-31.\n",
      "All data for CIPLA.NS has already been collected up to 2023-12-31.\n",
      "All data for ADANIGREEN.NS has already been collected up to 2023-12-31.\n",
      "All data for ADANIPORTS.NS has already been collected up to 2023-12-31.\n",
      "All data for HEROMOTOCO.NS has already been collected up to 2023-12-31.\n",
      "All data for COALINDIA.NS has already been collected up to 2023-12-31.\n",
      "All data for BPCL.NS has already been collected up to 2023-12-31.\n",
      "All data for APOLLOHOSP.NS has already been collected up to 2023-12-31.\n",
      "All data for TATAMOTORS.NS has already been collected up to 2023-12-31.\n",
      "Starting tweet collection for HDFCLIFE.NS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Tweets: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:05<00:00,  5.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished collecting tweets for HDFCLIFE.NS.\n",
      "Starting tweet collection for DABUR.NS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Tweets: 100%|█████████████████████████████████████████████████████████████| 278/278 [29:09<00:00,  6.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished collecting tweets for DABUR.NS.\n",
      "Starting tweet collection for BAJAJ-AUTO.NS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Tweets: 100%|█████████████████████████████████████████████████████████████| 730/730 [51:49<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished collecting tweets for BAJAJ-AUTO.NS.\n",
      "Starting tweet collection for DRREDDY.NS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Tweets: 100%|███████████████████████████████████████████████████████████| 730/730 [1:02:22<00:00,  5.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished collecting tweets for DRREDDY.NS.\n",
      "Starting tweet collection for SBILIFE.NS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Tweets: 100%|███████████████████████████████████████████████████████████| 730/730 [1:02:44<00:00,  5.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished collecting tweets for SBILIFE.NS.\n",
      "Starting tweet collection for EICHERMOT.NS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Tweets: 100%|█████████████████████████████████████████████████████████████| 730/730 [53:02<00:00,  4.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished collecting tweets for EICHERMOT.NS.\n",
      "Starting tweet collection for HINDALCO.NS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Tweets: 100%|███████████████████████████████████████████████████████████| 730/730 [1:02:34<00:00,  5.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished collecting tweets for HINDALCO.NS.\n",
      "Starting tweet collection for ICICIGI.NS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Tweets: 100%|███████████████████████████████████████████████████████████| 730/730 [1:02:41<00:00,  5.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished collecting tweets for ICICIGI.NS.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from scraper import scrape_tweets_parallel, get_last_collected_date\n",
    "\n",
    "# Configuration\n",
    "START_DATE = datetime(2022, 1, 1)  # Start date of the range\n",
    "END_DATE = datetime(2023, 12, 31)  # End date of the range\n",
    "STOCKS = [\"ITC.NS\", \"SBIN.NS\", \"KOTAKBANK.NS\", \"BHARTIARTL.NS\", \"HCLTECH.NS\", \"LT.NS\", \"AXISBANK.NS\",\n",
    "          \"ASIANPAINT.NS\", \"BAJFINANCE.NS\", \"MARUTI.NS\", \"M&M.NS\", \"SUNPHARMA.NS\", \"TITAN.NS\", \"ULTRACEMCO.NS\",\n",
    "          \"NESTLEIND.NS\", \"INDUSINDBK.NS\", \"ADANIENT.NS\", \"POWERGRID.NS\", \"NTPC.NS\", \"TATASTEEL.NS\", \"JSWSTEEL.NS\",\n",
    "          \"ONGC.NS\", \"BAJAJFINSV.NS\", \"DIVISLAB.NS\", \"TECHM.NS\", \"WIPRO.NS\", \"GRASIM.NS\", \"BRITANNIA.NS\", \n",
    "          \"CIPLA.NS\", \"ADANIGREEN.NS\", \"ADANIPORTS.NS\", \"HEROMOTOCO.NS\", \"COALINDIA.NS\", \"BPCL.NS\", \n",
    "          \"APOLLOHOSP.NS\", \"TATAMOTORS.NS\", \"HDFCLIFE.NS\", \"DABUR.NS\", \"BAJAJ-AUTO.NS\", \"DRREDDY.NS\", \n",
    "          \"SBILIFE.NS\", \"EICHERMOT.NS\", \"HINDALCO.NS\", \"ICICIGI.NS\"]\n",
    "\n",
    "QUERY_BASE_TEMPLATE = '{} OR Stock OR Market OR #StockMarket OR #MarketAnalysis'  # Base query\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='stock_tweet_collection.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "for stock in STOCKS:\n",
    "    CSV_FILE = f'{stock.replace(\".NS\", \"\")}.csv'  # Create a unique CSV file for each stock\n",
    "    QUERY_BASE = QUERY_BASE_TEMPLATE.format(stock)\n",
    "    \n",
    "    # Get the starting date\n",
    "    start_date = get_last_collected_date(CSV_FILE, START_DATE)\n",
    "\n",
    "    # Check if there's anything left to collect\n",
    "    if start_date > END_DATE:\n",
    "        print(f'All data for {stock} has already been collected up to {END_DATE.strftime(\"%Y-%m-%d\")}.')\n",
    "        continue\n",
    "\n",
    "    # Parallelize the collection of tweets for the specified date range\n",
    "    print(f'Starting tweet collection for {stock}...')\n",
    "    scrape_tweets_parallel(QUERY_BASE, start_date, END_DATE, CSV_FILE)\n",
    "\n",
    "    print(f'Finished collecting tweets for {stock}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f6c60a-9f95-4d49-9f4f-112452845ae1",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be650a0-d5c5-4daf-bd80-d4a0e3c25bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No issues found in ITC.csv.\n",
      "No issues found in SBIN.csv.\n",
      "No issues found in KOTAKBANK.csv.\n",
      "No issues found in BHARTIARTL.csv.\n",
      "No issues found in HCLTECH.csv.\n",
      "No issues found in LT.csv.\n",
      "No issues found in AXISBANK.csv.\n",
      "No issues found in ASIANPAINT.csv.\n",
      "No issues found in BAJFINANCE.csv.\n",
      "No issues found in MARUTI.csv.\n",
      "No issues found in M&M.csv.\n",
      "No issues found in SUNPHARMA.csv.\n",
      "No issues found in TITAN.csv.\n",
      "No issues found in ULTRACEMCO.csv.\n",
      "No issues found in NESTLEIND.csv.\n",
      "No issues found in INDUSINDBK.csv.\n",
      "No issues found in ADANIENT.csv.\n",
      "No issues found in POWERGRID.csv.\n",
      "No issues found in NTPC.csv.\n",
      "No issues found in TATASTEEL.csv.\n",
      "No issues found in JSWSTEEL.csv.\n",
      "No issues found in ONGC.csv.\n",
      "No issues found in BAJAJFINSV.csv.\n",
      "No issues found in DIVISLAB.csv.\n",
      "No issues found in TECHM.csv.\n",
      "No issues found in WIPRO.csv.\n",
      "No issues found in GRASIM.csv.\n",
      "No issues found in BRITANNIA.csv.\n",
      "No issues found in CIPLA.csv.\n",
      "No issues found in ADANIGREEN.csv.\n",
      "No issues found in ADANIPORTS.csv.\n",
      "No issues found in HEROMOTOCO.csv.\n",
      "No issues found in COALINDIA.csv.\n",
      "No issues found in BPCL.csv.\n",
      "No issues found in APOLLOHOSP.csv.\n",
      "No issues found in TATAMOTORS.csv.\n",
      "No issues found in HDFCLIFE.csv.\n",
      "File DABUR.csv does not exist.\n",
      "File BAJAJ-AUTO.csv does not exist.\n",
      "File DRREDDY.csv does not exist.\n",
      "File SBILIFE.csv does not exist.\n",
      "File EICHERMOT.csv does not exist.\n",
      "File HINDALCO.csv does not exist.\n",
      "File ICICIGI.csv does not exist.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Function to clean data by removing rows where column names appear in rows\n",
    "def clean_data(file_path):\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Find rows where the values match the column names (e.g., duplicates of column headers in the data)\n",
    "        header_row_mask = (df == df.columns).all(axis=1)\n",
    "        \n",
    "        # If such rows are found, remove them\n",
    "        if header_row_mask.any():\n",
    "            print(f\"Cleaning file: {file_path}\")\n",
    "            df_cleaned = df[~header_row_mask]  # Keep rows where the header doesn't match\n",
    "            \n",
    "            # Overwrite the original file with the cleaned data\n",
    "            df_cleaned.to_csv(file_path, index=False)\n",
    "            print(f\"File {file_path} cleaned and overwritten.\")\n",
    "        else:\n",
    "            print(f\"No issues found in {file_path}.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# List of stock symbols\n",
    "STOCKS = [\"ITC\", \"SBIN\", \"KOTAKBANK\", \"BHARTIARTL\", \"HCLTECH\", \"LT\", \"AXISBANK\",\n",
    "          \"ASIANPAINT\", \"BAJFINANCE\", \"MARUTI\", \"M&M\", \"SUNPHARMA\", \"TITAN\", \n",
    "          \"ULTRACEMCO\", \"NESTLEIND\", \"INDUSINDBK\", \"ADANIENT\", \"POWERGRID\", \n",
    "          \"NTPC\", \"TATASTEEL\", \"JSWSTEEL\", \"ONGC\", \"BAJAJFINSV\", \"DIVISLAB\", \n",
    "          \"TECHM\", \"WIPRO\", \"GRASIM\", \"BRITANNIA\", \"CIPLA\", \"ADANIGREEN\", \n",
    "          \"ADANIPORTS\", \"HEROMOTOCO\", \"COALINDIA\", \"BPCL\", \"APOLLOHOSP\", \n",
    "          \"TATAMOTORS\", \"HDFCLIFE\", \"DABUR\", \"BAJAJ-AUTO\", \"DRREDDY\", \n",
    "          \"SBILIFE\", \"EICHERMOT\", \"HINDALCO\", \"ICICIGI\"]\n",
    "\n",
    "# Loop through each stock symbol and clean the corresponding CSV file\n",
    "for stock in STOCKS:\n",
    "    file_name = f\"{stock}.csv\"\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_name):\n",
    "        clean_data(file_name)\n",
    "    else:\n",
    "        print(f\"File {file_name} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8b463d-f5d6-4053-ba24-f4375f731548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
